{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bagging and random forest from scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bagging for classification\n",
    "First, we can use the\n",
    "make classification() function to create a synthetic binary classification problem with 1,000\n",
    "examples and 20 input features. The complete example is listed below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 20) (1000,)\n"
     ]
    }
   ],
   "source": [
    "# synthetic binary classification dataset\n",
    "from sklearn.datasets import make_classification\n",
    "# define dataset\n",
    "X, y = make_classification(n_samples=1000, n_features=20, n_informative=15, n_redundant=5,\n",
    "random_state=5)\n",
    "# summarize the dataset\n",
    "print(X.shape, y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can  use the Bagging model as a final model and make predictions for classification.\n",
    "First, the Bagging ensemble is fit on all available data, then the predict() function can be\n",
    "called to make predictions on new data. The example below demonstrates this on our binary\n",
    "classification dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Class: 1\n"
     ]
    }
   ],
   "source": [
    "# make predictions using bagging for classification\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "# define dataset\n",
    "X, y = make_classification(n_samples=1000, n_features=20, n_informative=15, n_redundant=5,\n",
    "random_state=5)\n",
    "# define the model\n",
    "model = BaggingClassifier()\n",
    "# fit the model on the whole dataset\n",
    "model.fit(X, y)\n",
    "# make a single prediction\n",
    "row = [-4.7705504, -1.88685058, -0.96057964, 2.53850317, -6.5843005, 3.45711663,\n",
    "-7.46225013, 2.01338213, -0.45086384, -1.89314931, -2.90675203, -0.21214568,\n",
    "-0.9623956, 3.93862591, 0.06276375, 0.33964269, 4.0835676, 1.31423977, -2.17983117,\n",
    "3.1047287]\n",
    "yhat = model.predict([row])\n",
    "# summarize the prediction\n",
    "print('Predicted Class: %d' % yhat[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Bagging for regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we can use the\n",
    "make regression() function to create a synthetic regression problem with 1,000 examples and\n",
    "20 input features. The complete example is listed below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 20) (1000,)\n"
     ]
    }
   ],
   "source": [
    "# synthetic regression dataset\n",
    "from sklearn.datasets import make_regression\n",
    "# define dataset\n",
    "X, y = make_regression(n_samples=1000, n_features=20, n_informative=15, noise=0.1,\n",
    "random_state=5)\n",
    "# summarize the dataset\n",
    "print(X.shape, y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also use the Bagging model as a final model and make predictions for regression.\n",
    "First, the Bagging ensemble is fit on all available data, then the predict() function can be\n",
    "called to make predictions on new data. The example below demonstrates this on our regression\n",
    "dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: -190\n"
     ]
    }
   ],
   "source": [
    "# bagging ensemble for making predictions for regression\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.ensemble import BaggingRegressor\n",
    "# define dataset\n",
    "X, y = make_regression(n_samples=1000, n_features=20, n_informative=15, noise=0.1,\n",
    "random_state=5)\n",
    "# define the model\n",
    "model = BaggingRegressor()\n",
    "# fit the model on the whole dataset\n",
    "model.fit(X, y)\n",
    "# make a single prediction\n",
    "row = [0.88950817, -0.93540416, 0.08392824, 0.26438806, -0.52828711, -1.21102238,\n",
    "-0.4499934, 1.47392391, -0.19737726, -0.22252503, 0.02307668, 0.26953276, 0.03572757,\n",
    "-0.51606983, -0.39937452, 1.8121736, -0.00775917, -0.02514283, -0.76089365, 1.58692212]\n",
    "yhat = model.predict([row])\n",
    "# summarize the prediction\n",
    "print('Prediction: %d' % yhat[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpretation\n",
    "\n",
    "A high information gain suggests that splitting the dataset based on the chosen feature effectively reduces uncertainty about the class labels. Features with higher information gain are considered more informative and are preferred in decision tree algorithms and other machine learning models.\n",
    "\n",
    "## Application\n",
    "\n",
    "Information gain is widely used in decision tree algorithms, such as ID3 (Iterative Dichotomiser 3) and C4.5, to select the best features for splitting nodes in the tree. By recursively choosing features with the highest information gain, decision trees can efficiently partition the feature space and make accurate predictions.\n",
    "\n",
    "In summary, information gain plays a crucial role in feature selection and decision making in machine learning by quantifying the usefulness of features in reducing uncertainty about class labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def entropy(p):\n",
    "    if p == 0:\n",
    "        return 0\n",
    "    elif p == 1:\n",
    "        return 0\n",
    "    else:\n",
    "        return - (p * np.log2(p) + (1 - p) * np.log2(1-p))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def information_gain(left_child, right_child):\n",
    "    parent = ______ + ______\n",
    "    p_parent = parent.count(1) / len(parent) if len(parent) > 0 else 0\n",
    "    p_left = left_child.count(1) / len(left_child) if len(left_child) > 0 else 0\n",
    "    p_right = right_child.count(1) / len(right_child) if len(right_child) > 0 else 0\n",
    "    IG_p = ______(p_parent)\n",
    "    IG_l = ______(p_left)\n",
    "    IG_r = ______(p_right)\n",
    "    return IG_p - len(left_child) / len(parent) * IG_l - len(right_child) / len(parent) * IG_r"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5><font color=blue>Check result by executing below... 📝</font></h5>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%ipytest -qq\n",
    "def test_information_gain_basic():\n",
    "    # 基本情况：左右子节点都含有数据\n",
    "    left_child = [1, 0, 1, 0, 1]  # 示例左子节点\n",
    "    right_child = [0, 1, 0, 1, 0]  # 示例右子节点\n",
    "    assert information_gain(left_child, right_child) == pytest.approx(0.0)\n",
    "\n",
    "def test_information_gain_empty_left():\n",
    "    # 左子节点为空的情况\n",
    "    left_child = []  # 示例左子节点为空\n",
    "    right_child = [0, 1, 0, 1, 0]  # 示例右子节点\n",
    "    assert information_gain(left_child, right_child) == pytest.approx(0.0)\n",
    "\n",
    "def test_information_gain_empty_right():\n",
    "    # 右子节点为空的情况\n",
    "    left_child = [1, 0, 1, 0, 1]  # 示例左子节点\n",
    "    right_child = []  # 示例右子节点为空\n",
    "    assert information_gain(left_child, right_child) == pytest.approx(0.0)\n",
    "\n",
    "def test_information_gain_both_empty():\n",
    "    # 左右子节点均为空的情况\n",
    "    left_child = []  # 示例左子节点为空\n",
    "    right_child = []  # 示例右子节点为空\n",
    "    assert information_gain(left_child, right_child) == pytest.approx(0.0)\n",
    "\n",
    "def test_information_gain_extreme_values():\n",
    "    # 边缘情况：当p等于0或1时的情况\n",
    "    left_child = [1, 1, 1, 1]  # 示例左子节点全部为1\n",
    "    right_child = [0, 0, 0, 0]  # 示例右子节点全部为0\n",
    "    assert information_gain(left_child, right_child) == pytest.approx(1.0)\n",
    "\n",
    "    left_child = [0, 0, 0, 0]  # 示例左子节点全部为0\n",
    "    right_child = [1, 1, 1, 1]  # 示例右子节点全部为1\n",
    "    assert information_gain(left_child, right_child) == pytest.approx(1.0)\n",
    "\n",
    "    left_child = [1, 1, 1, 1]  # 示例左子节点全部为1\n",
    "    right_child = []  # 示例右子节点为空\n",
    "    assert information_gain(left_child, right_child) == pytest.approx(0.0)\n",
    "\n",
    "    left_child = []  # 示例左子节点为空\n",
    "    right_child = [1, 1, 1, 1]  # 示例右子节点全部为1\n",
    "    assert information_gain(left_child, right_child) == pytest.approx(0.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "    \n",
    "<details><summary>👩‍💻 <b>Hint</b></summary>\n",
    "\n",
    "You can consider to fill <code>left_child</code> and <code>right_child</code>.\n",
    "\n",
    "</details>\n",
    "\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
