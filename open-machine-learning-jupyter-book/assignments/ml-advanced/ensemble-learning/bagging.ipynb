{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bagging and random forest from scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bagging for classification\n",
    "First, we can use the\n",
    "make classification() function to create a synthetic binary classification problem with 1,000\n",
    "examples and 20 input features. The complete example is listed below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 20) (1000,)\n"
     ]
    }
   ],
   "source": [
    "# synthetic binary classification dataset\n",
    "from sklearn.datasets import make_classification\n",
    "# define dataset\n",
    "X, y = make_classification(n_samples=1000, n_features=20, n_informative=15, n_redundant=5,\n",
    "random_state=5)\n",
    "# summarize the dataset\n",
    "print(X.shape, y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can  use the Bagging model as a final model and make predictions for classification.\n",
    "First, the Bagging ensemble is fit on all available data, then the predict() function can be\n",
    "called to make predictions on new data. The example below demonstrates this on our binary\n",
    "classification dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Class: 1\n"
     ]
    }
   ],
   "source": [
    "# make predictions using bagging for classification\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "# define dataset\n",
    "X, y = make_classification(n_samples=1000, n_features=20, n_informative=15, n_redundant=5,\n",
    "random_state=5)\n",
    "# define the model\n",
    "model = BaggingClassifier()\n",
    "# fit the model on the whole dataset\n",
    "model.fit(X, y)\n",
    "# make a single prediction\n",
    "row = [-4.7705504, -1.88685058, -0.96057964, 2.53850317, -6.5843005, 3.45711663,\n",
    "-7.46225013, 2.01338213, -0.45086384, -1.89314931, -2.90675203, -0.21214568,\n",
    "-0.9623956, 3.93862591, 0.06276375, 0.33964269, 4.0835676, 1.31423977, -2.17983117,\n",
    "3.1047287]\n",
    "yhat = model.predict([row])\n",
    "# summarize the prediction\n",
    "print('Predicted Class: %d' % yhat[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Bagging for regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we can use the\n",
    "make regression() function to create a synthetic regression problem with 1,000 examples and\n",
    "20 input features. The complete example is listed below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 20) (1000,)\n"
     ]
    }
   ],
   "source": [
    "# synthetic regression dataset\n",
    "from sklearn.datasets import make_regression\n",
    "# define dataset\n",
    "X, y = make_regression(n_samples=1000, n_features=20, n_informative=15, noise=0.1,\n",
    "random_state=5)\n",
    "# summarize the dataset\n",
    "print(X.shape, y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also use the Bagging model as a final model and make predictions for regression.\n",
    "First, the Bagging ensemble is fit on all available data, then the predict() function can be\n",
    "called to make predictions on new data. The example below demonstrates this on our regression\n",
    "dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: -190\n"
     ]
    }
   ],
   "source": [
    "# bagging ensemble for making predictions for regression\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.ensemble import BaggingRegressor\n",
    "# define dataset\n",
    "X, y = make_regression(n_samples=1000, n_features=20, n_informative=15, noise=0.1,\n",
    "random_state=5)\n",
    "# define the model\n",
    "model = BaggingRegressor()\n",
    "# fit the model on the whole dataset\n",
    "model.fit(X, y)\n",
    "# make a single prediction\n",
    "row = [0.88950817, -0.93540416, 0.08392824, 0.26438806, -0.52828711, -1.21102238,\n",
    "-0.4499934, 1.47392391, -0.19737726, -0.22252503, 0.02307668, 0.26953276, 0.03572757,\n",
    "-0.51606983, -0.39937452, 1.8121736, -0.00775917, -0.02514283, -0.76089365, 1.58692212]\n",
    "yhat = model.predict([row])\n",
    "# summarize the prediction\n",
    "print('Prediction: %d' % yhat[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpretation\n",
    "\n",
    "A high information gain suggests that splitting the dataset based on the chosen feature effectively reduces uncertainty about the class labels. Features with higher information gain are considered more informative and are preferred in decision tree algorithms and other machine learning models.\n",
    "\n",
    "## Application\n",
    "\n",
    "Information gain is widely used in decision tree algorithms, such as ID3 (Iterative Dichotomiser 3) and C4.5, to select the best features for splitting nodes in the tree. By recursively choosing features with the highest information gain, decision trees can efficiently partition the feature space and make accurate predictions.\n",
    "\n",
    "In summary, information gain plays a crucial role in feature selection and decision making in machine learning by quantifying the usefulness of features in reducing uncertainty about class labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def entropy(p):\n",
    "    if p == 0:\n",
    "        return 0\n",
    "    elif p == 1:\n",
    "        return 0\n",
    "    else:\n",
    "        return - (p * np.log2(p) + (1 - p) * np.log2(1-p))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def information_gain(left_child, right_child):\n",
    "    parent = ______ + ______\n",
    "    p_parent = parent.count(1) / len(parent) if len(parent) > 0 else 0\n",
    "    p_left = left_child.count(1) / len(left_child) if len(left_child) > 0 else 0\n",
    "    p_right = right_child.count(1) / len(right_child) if len(right_child) > 0 else 0\n",
    "    IG_p = ______(p_parent)\n",
    "    IG_l = ______(p_left)\n",
    "    IG_r = ______(p_right)\n",
    "    return IG_p - len(left_child) / len(parent) * IG_l - len(right_child) / len(parent) * IG_r"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5><font color=blue>Check result by executing below... ğŸ“</font></h5>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%ipytest -qq\n",
    "def test_information_gain_basic():\n",
    "    # åŸºæœ¬æƒ…å†µï¼šå·¦å³å­èŠ‚ç‚¹éƒ½å«æœ‰æ•°æ®\n",
    "    left_child = [1, 0, 1, 0, 1]  # ç¤ºä¾‹å·¦å­èŠ‚ç‚¹\n",
    "    right_child = [0, 1, 0, 1, 0]  # ç¤ºä¾‹å³å­èŠ‚ç‚¹\n",
    "    assert information_gain(left_child, right_child) == pytest.approx(0.0)\n",
    "\n",
    "def test_information_gain_empty_left():\n",
    "    # å·¦å­èŠ‚ç‚¹ä¸ºç©ºçš„æƒ…å†µ\n",
    "    left_child = []  # ç¤ºä¾‹å·¦å­èŠ‚ç‚¹ä¸ºç©º\n",
    "    right_child = [0, 1, 0, 1, 0]  # ç¤ºä¾‹å³å­èŠ‚ç‚¹\n",
    "    assert information_gain(left_child, right_child) == pytest.approx(0.0)\n",
    "\n",
    "def test_information_gain_empty_right():\n",
    "    # å³å­èŠ‚ç‚¹ä¸ºç©ºçš„æƒ…å†µ\n",
    "    left_child = [1, 0, 1, 0, 1]  # ç¤ºä¾‹å·¦å­èŠ‚ç‚¹\n",
    "    right_child = []  # ç¤ºä¾‹å³å­èŠ‚ç‚¹ä¸ºç©º\n",
    "    assert information_gain(left_child, right_child) == pytest.approx(0.0)\n",
    "\n",
    "def test_information_gain_both_empty():\n",
    "    # å·¦å³å­èŠ‚ç‚¹å‡ä¸ºç©ºçš„æƒ…å†µ\n",
    "    left_child = []  # ç¤ºä¾‹å·¦å­èŠ‚ç‚¹ä¸ºç©º\n",
    "    right_child = []  # ç¤ºä¾‹å³å­èŠ‚ç‚¹ä¸ºç©º\n",
    "    assert information_gain(left_child, right_child) == pytest.approx(0.0)\n",
    "\n",
    "def test_information_gain_extreme_values():\n",
    "    # è¾¹ç¼˜æƒ…å†µï¼šå½“pç­‰äº0æˆ–1æ—¶çš„æƒ…å†µ\n",
    "    left_child = [1, 1, 1, 1]  # ç¤ºä¾‹å·¦å­èŠ‚ç‚¹å…¨éƒ¨ä¸º1\n",
    "    right_child = [0, 0, 0, 0]  # ç¤ºä¾‹å³å­èŠ‚ç‚¹å…¨éƒ¨ä¸º0\n",
    "    assert information_gain(left_child, right_child) == pytest.approx(1.0)\n",
    "\n",
    "    left_child = [0, 0, 0, 0]  # ç¤ºä¾‹å·¦å­èŠ‚ç‚¹å…¨éƒ¨ä¸º0\n",
    "    right_child = [1, 1, 1, 1]  # ç¤ºä¾‹å³å­èŠ‚ç‚¹å…¨éƒ¨ä¸º1\n",
    "    assert information_gain(left_child, right_child) == pytest.approx(1.0)\n",
    "\n",
    "    left_child = [1, 1, 1, 1]  # ç¤ºä¾‹å·¦å­èŠ‚ç‚¹å…¨éƒ¨ä¸º1\n",
    "    right_child = []  # ç¤ºä¾‹å³å­èŠ‚ç‚¹ä¸ºç©º\n",
    "    assert information_gain(left_child, right_child) == pytest.approx(0.0)\n",
    "\n",
    "    left_child = []  # ç¤ºä¾‹å·¦å­èŠ‚ç‚¹ä¸ºç©º\n",
    "    right_child = [1, 1, 1, 1]  # ç¤ºä¾‹å³å­èŠ‚ç‚¹å…¨éƒ¨ä¸º1\n",
    "    assert information_gain(left_child, right_child) == pytest.approx(0.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "    \n",
    "<details><summary>ğŸ‘©â€ğŸ’» <b>Hint</b></summary>\n",
    "\n",
    "You can consider to fill <code>left_child</code> and <code>right_child</code>.\n",
    "\n",
    "</details>\n",
    "\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
